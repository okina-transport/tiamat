package org.rutebanken.tiamat.rest.netex.publicationdelivery;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import com.hazelcast.core.HazelcastInstance;
import org.rutebanken.netex.model.SiteFrame;
import org.rutebanken.netex.model.StopPlace;
import org.rutebanken.tiamat.importer.initial.InitialStopPlaceImporter;
import org.rutebanken.tiamat.netex.mapping.NetexMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.MDC;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;
import org.xml.sax.SAXException;

import javax.ws.rs.Consumes;
import javax.ws.rs.POST;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import javax.xml.bind.JAXBException;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.stream.XMLStreamException;
import java.io.IOException;
import java.io.InputStream;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.Lock;

import static org.rutebanken.tiamat.importer.PublicationDeliveryImporter.IMPORT_CORRELATION_ID;

@Component
@Produces("application/xml")
@Path("/publication_delivery")
public class InitialImportResource {

    private static final Logger logger = LoggerFactory.getLogger(InitialImportResource.class);
    private static final String KEY_INITIAL_IMPORT_LOCK = "initial_import_lock";

    private final PublicationDeliveryPartialUnmarshaller publicationDeliveryPartialUnmarshaller;
    private final NetexMapper netexMapper;
    private final InitialStopPlaceImporter initialStopPlaceImporter;
    private final HazelcastInstance hazelcastInstance;

    @Autowired
    public InitialImportResource(PublicationDeliveryPartialUnmarshaller publicationDeliveryPartialUnmarshaller, NetexMapper netexMapper, InitialStopPlaceImporter initialStopPlaceImporter, HazelcastInstance hazelcastInstance) {
        this.publicationDeliveryPartialUnmarshaller = publicationDeliveryPartialUnmarshaller;
        this.netexMapper = netexMapper;
        this.initialStopPlaceImporter = initialStopPlaceImporter;
        this.hazelcastInstance = hazelcastInstance;
    }

    /**
     * This method requires all incoming data to have IDs that are previously generated by Tiamat and that they are unique.
     * IDs for quays and stop places will not be generated. They will be used as is.
     * TODO: Move this to PublicationDeliveryImporter class
     */
    @POST
    @Path("initial_import")
    @Consumes(MediaType.APPLICATION_XML)
    @Produces(MediaType.TEXT_PLAIN)
    public Response importPublicationDeliveryOnEmptyDatabase(InputStream inputStream) throws IOException, JAXBException, SAXException, XMLStreamException, InterruptedException, ParserConfigurationException {

        Lock lock = hazelcastInstance.getLock(KEY_INITIAL_IMPORT_LOCK);

        if (lock.tryLock()) {
            int threads = Runtime.getRuntime().availableProcessors();
            ExecutorService executorService = Executors.newFixedThreadPool(threads, new ThreadFactoryBuilder().setNameFormat("importer-%d").build());

            try {
                UnmarshalResult unmarshalResult = publicationDeliveryPartialUnmarshaller.unmarshal(inputStream);
                AtomicInteger topographicPlacesCounter = new AtomicInteger();
                org.rutebanken.tiamat.model.SiteFrame siteFrame = unmarshalResult.getPublicationDeliveryStructure().getDataObjects().getCompositeFrameOrCommonFrame()
                        .stream()
                        .filter(element -> element.getValue() instanceof SiteFrame)
                        .map(element -> (SiteFrame) element.getValue())
                        .peek(netexSiteFrame -> {
                            MDC.put(IMPORT_CORRELATION_ID, netexSiteFrame.getId());
                            logger.info("Publication delivery contains site frame created at {}", netexSiteFrame.getCreated());
                        })
                        .map(netexSiteFrame -> netexMapper.mapToTiamatModel(netexSiteFrame))
                        .findFirst().get();


                logger.info("Importing stops");

                AtomicInteger stopPlacesImported = new AtomicInteger(0);

                AtomicBoolean stopExecution = new AtomicBoolean(false);

                for(int i = 0; i < threads; i++) {
                    executorService.submit(() -> {
                        try {
                            while (!Thread.currentThread().isInterrupted() && !stopExecution.get()) {
                                StopPlace stopPlace = unmarshalResult.getStopPlaceQueue().take();

                                if (stopPlace.getId().equals(RunnableUnmarshaller.POISON_STOP_PLACE.getId())) {
                                    logger.info("Finished importing stops");
                                    stopExecution.set(true);
                                    break;
                                }

                                initialStopPlaceImporter.importStopPlace(stopPlacesImported,
                                        topographicPlacesCounter, siteFrame, netexMapper.mapToTiamatModel(stopPlace));
                            }
                        } catch (InterruptedException e) {
                            Thread.currentThread().interrupt();
                            stopExecution.set(true);
                            return;
                        } catch (ExecutionException e) {
                            logger.warn("Execution exception", e);
                            stopExecution.set(true);
                        }
                    });
                }

                logger.info("Waiting for all import tasks to finish");

                executorService.shutdown();
                executorService.awaitTermination(150, TimeUnit.MINUTES);

                return Response.ok("Imported " + stopPlacesImported.get() + " stop places.").build();

            } catch (Exception e) {
                logger.error("Caught exception while importing publication delivery initially", e);
                executorService.shutdownNow();
                return Response.status(Response.Status.INTERNAL_SERVER_ERROR).entity("Caught exception while import publication delivery: " + e.getMessage()).build();
            } finally {
                lock.unlock();
            }
        }
        return Response.status(Response.Status.CONFLICT).entity("There is already an import job running").build();
    }
}
